{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U bitsandbytes\n",
    "%pip install -U transformers\n",
    "%pip install -U accelerate\n",
    "%pip install -U peft\n",
    "%pip install -U trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mqtra0027\u001b[0m (\u001b[33mailecs-lab-students\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/Pycharm Project/DUTA10K/wandb/run-20250512_123042-mabxyu2n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ailecs-lab-students/Using%20Llama3.2%20to%20classify%20illicit%20content%20on%20online%20marketplace_ver%205/runs/mabxyu2n?apiKey=7c62613817d0b4287c0beb0cfdd236bbb509d82f' target=\"_blank\">upbeat-shadow-3</a></strong> to <a href='https://wandb.ai/ailecs-lab-students/Using%20Llama3.2%20to%20classify%20illicit%20content%20on%20online%20marketplace_ver%205?apiKey=7c62613817d0b4287c0beb0cfdd236bbb509d82f' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ailecs-lab-students/Using%20Llama3.2%20to%20classify%20illicit%20content%20on%20online%20marketplace_ver%205?apiKey=7c62613817d0b4287c0beb0cfdd236bbb509d82f' target=\"_blank\">https://wandb.ai/ailecs-lab-students/Using%20Llama3.2%20to%20classify%20illicit%20content%20on%20online%20marketplace_ver%205?apiKey=7c62613817d0b4287c0beb0cfdd236bbb509d82f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ailecs-lab-students/Using%20Llama3.2%20to%20classify%20illicit%20content%20on%20online%20marketplace_ver%205/runs/mabxyu2n?apiKey=7c62613817d0b4287c0beb0cfdd236bbb509d82f' target=\"_blank\">https://wandb.ai/ailecs-lab-students/Using%20Llama3.2%20to%20classify%20illicit%20content%20on%20online%20marketplace_ver%205/runs/mabxyu2n?apiKey=7c62613817d0b4287c0beb0cfdd236bbb509d82f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Do NOT share these links with anyone. They can be used to claim your runs."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project='Using Llama3.2 to classify illicit content on online marketplace_ver 5', \n",
    "    job_type=\"training\", \n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "import evaluate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "from trl import setup_chat_format\n",
    "from transformers import (LlamaForSequenceClassification, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          DataCollatorWithPadding, \n",
    "                          EarlyStoppingCallback)\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                            precision_score, \n",
    "                            recall_score, \n",
    "                            f1_score, \n",
    "                            classification_report, \n",
    "                            confusion_matrix,\n",
    "                            precision_recall_fscore_support)\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import pipeline, Trainer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSONL file\n",
    "file_path = \"DUTA10K_final.jsonl\"\n",
    "df = pd.read_json(file_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 categories, e.g.: ['Art_Music', 'Casino_Gambling', 'Counterfeit Credit-Cards', 'Counterfeit Money', 'Counterfeit Personal-Identification_Driving-Licence'] → 0\n"
     ]
    }
   ],
   "source": [
    "# 1a) get sorted list of unique categories\n",
    "categories = sorted(df[\"category\"].unique())\n",
    "num_labels = len(categories)\n",
    "\n",
    "# 1b) build id2label / label2id\n",
    "id2label = {i: cat for i, cat in enumerate(categories)}\n",
    "label2id = {cat: i for i, cat in enumerate(categories)}\n",
    "\n",
    "print(f\"{num_labels} categories, e.g.: {categories[:5]} → {label2id[categories[0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Define split sizes\n",
    "train_size = 0.8\n",
    "eval_size = 0.1\n",
    "\n",
    "# Calculate split indices\n",
    "train_end = int(train_size * len(df))\n",
    "eval_end = train_end + int(eval_size * len(df))\n",
    "\n",
    "# Split the data\n",
    "df_train = df[:train_end].copy()\n",
    "df_eval = df[train_end:eval_end].copy()\n",
    "df_test = df[eval_end:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e1efe286f142d1a5d899f06e2117a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3342 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a0e52636064ba1a5df7a654c1ec171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/417 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5c842492bd466396c463ae9516ddf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/419 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train.reset_index(drop=True)),\n",
    "    \"eval\":  Dataset.from_pandas(df_eval.reset_index(drop=True)),\n",
    "    \"test\":  Dataset.from_pandas(df_test.reset_index(drop=True)),\n",
    "})\n",
    "\n",
    "# add a new “label_id” column\n",
    "def encode_label(ex):\n",
    "    return {\"labels\": label2id[ex[\"category\"]]}\n",
    "ds = ds.map(encode_label, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1303d5d96ab424f9900263848a1ac6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3342 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c97ade29df438d85337cc9fa8234c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/417 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b1546e85604b2d9b2725561176f944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/419 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "# Load model directly\n",
    "base_model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "MAX_LEN = 10000\n",
    "\n",
    "# Preprocess function to tokenize the text data\n",
    "def preprocess(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "# Apply preprocessing to the datasets and remove original columns\n",
    "ds = ds.remove_columns([\"source\", \"lang\", \"label\", \"category\"])  # drop old binary label if present\n",
    "tokenized = ds.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]      # now only input_ids, attention_mask, labels remain\n",
    ")\n",
    "\n",
    "# Set the format of the tokenized datasets to PyTorch tensors\n",
    "tokenized.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n",
    "\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute weights inversely proportional to class frequency\n",
    "y_train = np.array(ds[\"train\"][\"labels\"])\n",
    "cw = compute_class_weight(\"balanced\", classes=np.arange(num_labels), y=y_train)\n",
    "class_weights = torch.tensor(cw, dtype=torch.float)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the original forward method reference FIRST\n",
    "_orig_forward = LlamaForSequenceClassification.forward\n",
    "\n",
    "# Define the wrapper function that USES the _orig_forward reference\n",
    "def actual_forward_wrapper(\n",
    "    self,                   # Instance of the model\n",
    "    input_ids=None,\n",
    "    attention_mask=None,\n",
    "    labels=None,            # This will capture the labels passed by the Trainer\n",
    "    **kwargs\n",
    "):\n",
    "    # Remove Trainer’s extra argument if present\n",
    "    kwargs.pop(\"num_items_in_batch\", None)\n",
    "\n",
    "    # Call the *original* forward method (using the captured _orig_forward)\n",
    "    outputs = _orig_forward(\n",
    "        self,\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=None,  # Explicitly set labels to None for the base model call\n",
    "        **kwargs      # Pass any other captured kwargs\n",
    "    )\n",
    "    logits = outputs.logits  # (batch_size, num_labels)\n",
    "\n",
    "    loss = None\n",
    "    if labels is not None: # Use the 'labels' captured by *this* function's signature\n",
    "        # Ensure class weights are on the same device as logits\n",
    "        # Make sure 'class_weights' is accessible in this scope (defined outside)\n",
    "        loss_fct = CrossEntropyLoss(weight=class_weights.to(logits.device))\n",
    "        loss = loss_fct(logits.view(-1, self.config.num_labels),\n",
    "                        labels.view(-1))\n",
    "\n",
    "    # Return a full SequenceClassifierOutput\n",
    "    return SequenceClassifierOutput(\n",
    "        loss=loss,\n",
    "        logits=logits,\n",
    "        hidden_states=outputs.hidden_states,\n",
    "        attentions=outputs.attentions,\n",
    "    )\n",
    "\n",
    "# Replace the class's forward method with the wrapper\n",
    "LlamaForSequenceClassification.forward = actual_forward_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed73414b4e1c4d56a950eafda2895c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-3B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Configure BitsAndBytes for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load the LlamaForSequenceClassification model with quantization\n",
    "model = LlamaForSequenceClassification.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Set the padding token ID for the model configuration\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Prepare the model for k-bit training (LoRA compatible)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Enable gradient checkpointing to save memory during training\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure class weights are on the same device as the model parameters\n",
    "class_weights = class_weights.to(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 48,750,592 || all params: 3,261,623,296 || trainable%: 1.4947\n"
     ]
    }
   ],
   "source": [
    "# Identify target modules for LoRA adaptation (all linear layers)\n",
    "target_modules = [n.split(\".\")[-1]\n",
    "                  for n, m in model.named_modules()\n",
    "                  if isinstance(m, torch.nn.Linear)]\n",
    "\n",
    "# Configure LoRA (Low-Rank Adaptation)\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")\n",
    "\n",
    "# Get the PEFT (Parameter-Efficient Fine-Tuning) model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print the number of trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2212415/3674777632.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Function to compute evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"accuracy\":  acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\":    rec,\n",
    "        \"f1\":        f1,\n",
    "    }\n",
    "\n",
    "# Configure training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"llama3_multi_v1\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    num_train_epochs=8,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    report_to=[\"wandb\"],\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"eval\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3336' max='3336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3336/3336 32:25:11, Epoch 7/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>26.014800</td>\n",
       "      <td>4.220622</td>\n",
       "      <td>0.299760</td>\n",
       "      <td>0.275701</td>\n",
       "      <td>0.299760</td>\n",
       "      <td>0.270639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>20.535300</td>\n",
       "      <td>3.528178</td>\n",
       "      <td>0.417266</td>\n",
       "      <td>0.376999</td>\n",
       "      <td>0.417266</td>\n",
       "      <td>0.387599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>14.457600</td>\n",
       "      <td>3.157916</td>\n",
       "      <td>0.541966</td>\n",
       "      <td>0.571509</td>\n",
       "      <td>0.541966</td>\n",
       "      <td>0.530455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>11.129000</td>\n",
       "      <td>2.439070</td>\n",
       "      <td>0.613909</td>\n",
       "      <td>0.647024</td>\n",
       "      <td>0.613909</td>\n",
       "      <td>0.617243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>8.134400</td>\n",
       "      <td>2.141205</td>\n",
       "      <td>0.669065</td>\n",
       "      <td>0.684437</td>\n",
       "      <td>0.669065</td>\n",
       "      <td>0.666314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>7.734700</td>\n",
       "      <td>1.923116</td>\n",
       "      <td>0.712230</td>\n",
       "      <td>0.729562</td>\n",
       "      <td>0.712230</td>\n",
       "      <td>0.713446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.985200</td>\n",
       "      <td>1.969856</td>\n",
       "      <td>0.702638</td>\n",
       "      <td>0.708352</td>\n",
       "      <td>0.702638</td>\n",
       "      <td>0.701232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>4.072300</td>\n",
       "      <td>1.808722</td>\n",
       "      <td>0.709832</td>\n",
       "      <td>0.738096</td>\n",
       "      <td>0.709832</td>\n",
       "      <td>0.712802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.678400</td>\n",
       "      <td>1.796435</td>\n",
       "      <td>0.745803</td>\n",
       "      <td>0.765428</td>\n",
       "      <td>0.745803</td>\n",
       "      <td>0.746754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.987200</td>\n",
       "      <td>1.830853</td>\n",
       "      <td>0.755396</td>\n",
       "      <td>0.761030</td>\n",
       "      <td>0.755396</td>\n",
       "      <td>0.751125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.734200</td>\n",
       "      <td>1.823282</td>\n",
       "      <td>0.750600</td>\n",
       "      <td>0.760962</td>\n",
       "      <td>0.750600</td>\n",
       "      <td>0.749032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.935600</td>\n",
       "      <td>1.792783</td>\n",
       "      <td>0.769784</td>\n",
       "      <td>0.774887</td>\n",
       "      <td>0.769784</td>\n",
       "      <td>0.765537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.319900</td>\n",
       "      <td>1.867050</td>\n",
       "      <td>0.769784</td>\n",
       "      <td>0.772799</td>\n",
       "      <td>0.769784</td>\n",
       "      <td>0.764459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.361200</td>\n",
       "      <td>1.859577</td>\n",
       "      <td>0.769784</td>\n",
       "      <td>0.785372</td>\n",
       "      <td>0.769784</td>\n",
       "      <td>0.770330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.174100</td>\n",
       "      <td>1.891954</td>\n",
       "      <td>0.767386</td>\n",
       "      <td>0.772550</td>\n",
       "      <td>0.767386</td>\n",
       "      <td>0.763746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.164500</td>\n",
       "      <td>1.924180</td>\n",
       "      <td>0.772182</td>\n",
       "      <td>0.776096</td>\n",
       "      <td>0.772182</td>\n",
       "      <td>0.768532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3336, training_loss=6.98902466628763, metrics={'train_runtime': 116744.3564, 'train_samples_per_second': 0.229, 'train_steps_per_second': 0.029, 'total_flos': 4.58945513558016e+18, 'train_loss': 6.98902466628763, 'epoch': 7.981448234590066})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model on the test set and print classification report and confusion matrix\n",
    "def evaluate_on_test(trainer, test_dataset, id2label):\n",
    "    # Get predictions\n",
    "    preds_output = trainer.predict(test_dataset)\n",
    "    y_true = preds_output.label_ids\n",
    "    y_pred = np.argmax(preds_output.predictions, axis=-1)\n",
    "\n",
    "    # Which labels actually appear in the test set?\n",
    "    present_labels = sorted(set(y_true.tolist()))\n",
    "    present_names  = [id2label[i] for i in present_labels]\n",
    "\n",
    "    # Print report for only those classes\n",
    "    print(\"=== Classification Report ===\")\n",
    "    print(classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        labels=present_labels,\n",
    "        target_names=present_names,\n",
    "        zero_division=0,\n",
    "        digits=4\n",
    "    ))\n",
    "\n",
    "    # Confusion matrix (same subset of labels)\n",
    "    print(\"=== Confusion Matrix ===\")\n",
    "    print(confusion_matrix(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        labels=present_labels\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Classification Report ===\n",
      "                                              precision    recall  f1-score   support\n",
      "\n",
      "                                   Art_Music     1.0000    0.5000    0.6667         2\n",
      "                             Casino_Gambling     0.7500    0.6000    0.6667         5\n",
      "                    Counterfeit Credit-Cards     0.9048    0.9500    0.9268        20\n",
      "                           Counterfeit Money     1.0000    1.0000    1.0000         2\n",
      "Counterfeit Personal-Identification_Passport     1.0000    0.7500    0.8571         4\n",
      "                              Cryptocurrency     0.9623    0.9623    0.9623        53\n",
      "                                Cryptolocker     0.9412    1.0000    0.9697        16\n",
      "                               Drugs_Illegal     0.7586    0.8800    0.8148        25\n",
      "                               Forum_Illegal     0.5000    0.5000    0.5000         2\n",
      "                                 Forum_Legal     0.5455    0.6667    0.6000         9\n",
      "                                     Hacking     0.6923    0.6923    0.6923        13\n",
      "                           Hosting_Directory     1.0000    0.7500    0.8571         8\n",
      "                        Hosting_File-sharing     0.7273    0.6154    0.6667        13\n",
      "                             Hosting_Folders     0.2857    0.4000    0.3333         5\n",
      "                       Hosting_Search-Engine     0.4000    0.5000    0.4444         4\n",
      "                              Hosting_Server     0.9865    0.9605    0.9733        76\n",
      "                            Hosting_Software     0.6667    0.5600    0.6087        25\n",
      "                                 Leaked-Data     0.0000    0.0000    0.0000         1\n",
      "                               Library_Books     1.0000    0.5000    0.6667         2\n",
      "                         Marketplace_Illegal     0.6000    0.4286    0.5000         7\n",
      "                           Marketplace_Legal     1.0000    0.7143    0.8333         7\n",
      "                                    Personal     0.3913    0.6279    0.4821        43\n",
      "                                    Politics     0.0000    0.0000    0.0000         2\n",
      "                     Porno_Child-pornography     1.0000    0.3750    0.5455         8\n",
      "                   Porno_General-pornography     0.5556    0.7143    0.6250         7\n",
      "                              Services_Legal     0.2000    0.1538    0.1739        13\n",
      "                         Social-Network_Blog     0.5000    0.5000    0.5000        18\n",
      "                         Social-Network_Chat     0.7500    0.6000    0.6667         5\n",
      "                        Social-Network_Email     0.7500    0.8571    0.8000         7\n",
      "                         Social-Network_News     0.5000    0.1429    0.2222         7\n",
      "                               Violence_Hate     1.0000    0.4000    0.5714         5\n",
      "                             Violence_Hitman     0.0000    0.0000    0.0000         2\n",
      "                            Violence_Weapons     1.0000    1.0000    1.0000         3\n",
      "\n",
      "                                    accuracy                         0.7351       419\n",
      "                                   macro avg     0.6778    0.5849    0.6099       419\n",
      "                                weighted avg     0.7540    0.7351    0.7323       419\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "[[ 1  0  0 ...  0  0  0]\n",
      " [ 0  3  0 ...  0  0  0]\n",
      " [ 0  0 19 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  2  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  3]]\n"
     ]
    }
   ],
   "source": [
    "# Perform evaluation on the test dataset\n",
    "evaluate_on_test(trainer, tokenized[\"test\"], id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▃▅▆▆▇▇▇████████</td></tr><tr><td>eval/f1</td><td>▁▃▅▆▇▇▇▇████████</td></tr><tr><td>eval/loss</td><td>█▆▅▃▂▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/precision</td><td>▁▂▅▆▇▇▇▇████████</td></tr><tr><td>eval/recall</td><td>▁▃▅▆▆▇▇▇████████</td></tr><tr><td>eval/runtime</td><td>▃▂▆▃▅▁▁▁▁█▇█▆█▇▇</td></tr><tr><td>eval/samples_per_second</td><td>▆█▃▆▄████▁▁▁▃▁▁▁</td></tr><tr><td>eval/steps_per_second</td><td>██▁█▁████▁▁▁▁▁▁▁</td></tr><tr><td>test/accuracy</td><td>▁</td></tr><tr><td>test/f1</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>test/precision</td><td>▁</td></tr><tr><td>test/recall</td><td>▁</td></tr><tr><td>test/runtime</td><td>▁</td></tr><tr><td>test/samples_per_second</td><td>▁</td></tr><tr><td>test/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/grad_norm</td><td>▆█▅▅▄▅▅▄▅▄▄▄▅▅▂▆▇▅▄▄▅▂▄▅▃▁▂▂▂▅▁▁▁▂▁▃▂▁▁▄</td></tr><tr><td>train/learning_rate</td><td>▂▄▅▆▇█████▇▇▆▆▆▆▆▅▅▅▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▆▅▄▃▃▃▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.77218</td></tr><tr><td>eval/f1</td><td>0.76853</td></tr><tr><td>eval/loss</td><td>1.92418</td></tr><tr><td>eval/precision</td><td>0.7761</td></tr><tr><td>eval/recall</td><td>0.77218</td></tr><tr><td>eval/runtime</td><td>448.1879</td></tr><tr><td>eval/samples_per_second</td><td>0.93</td></tr><tr><td>eval/steps_per_second</td><td>0.118</td></tr><tr><td>test/accuracy</td><td>0.73508</td></tr><tr><td>test/f1</td><td>0.73227</td></tr><tr><td>test/loss</td><td>1.87672</td></tr><tr><td>test/precision</td><td>0.75398</td></tr><tr><td>test/recall</td><td>0.73508</td></tr><tr><td>test/runtime</td><td>450.1922</td></tr><tr><td>test/samples_per_second</td><td>0.931</td></tr><tr><td>test/steps_per_second</td><td>0.118</td></tr><tr><td>total_flos</td><td>4.58945513558016e+18</td></tr><tr><td>train/epoch</td><td>7.98145</td></tr><tr><td>train/global_step</td><td>3336</td></tr><tr><td>train/grad_norm</td><td>200.68945</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4564</td></tr><tr><td>train_loss</td><td>6.98902</td></tr><tr><td>train_runtime</td><td>116744.3564</td></tr><tr><td>train_samples_per_second</td><td>0.229</td></tr><tr><td>train_steps_per_second</td><td>0.029</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">upbeat-shadow-3</strong> at: <a href='https://wandb.ai/ailecs-lab-students/Using%20Llama3.2%20to%20classify%20illicit%20content%20on%20online%20marketplace_ver%205/runs/mabxyu2n?apiKey=7c62613817d0b4287c0beb0cfdd236bbb509d82f' target=\"_blank\">https://wandb.ai/ailecs-lab-students/Using%20Llama3.2%20to%20classify%20illicit%20content%20on%20online%20marketplace_ver%205/runs/mabxyu2n?apiKey=7c62613817d0b4287c0beb0cfdd236bbb509d82f</a><br> View project at: <a href='https://wandb.ai/ailecs-lab-students/Using%20Llama3.2%20to%20classify%20illicit%20content%20on%20online%20marketplace_ver%205?apiKey=7c62613817d0b4287c0beb0cfdd236bbb509d82f' target=\"_blank\">https://wandb.ai/ailecs-lab-students/Using%20Llama3.2%20to%20classify%20illicit%20content%20on%20online%20marketplace_ver%205?apiKey=7c62613817d0b4287c0beb0cfdd236bbb509d82f</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250512_123042-mabxyu2n/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finish the Weights & Biases run\n",
    "wandb.finish()\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('llama-3.2-fine-tuned-model_ver5/tokenizer_config.json',\n",
       " 'llama-3.2-fine-tuned-model_ver5/special_tokens_map.json',\n",
       " 'llama-3.2-fine-tuned-model_ver5/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save trained model and tokenizer\n",
    "trainer.save_model(\"llama-3.2-fine-tuned-model_ver5\")\n",
    "tokenizer.save_pretrained(\"llama-3.2-fine-tuned-model_ver5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from peft import PeftModel\n",
    "\n",
    "# 🔐 Login to HuggingFace\n",
    "from getpass import getpass\n",
    "hf_token = getpass(\"Enter your HuggingFace token: \")\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading base tokenizer and model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190499ba6d7d4892988dcac70c33cd1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 🧠 Base and fine-tuned model paths\n",
    "base_model = \"meta-llama/Llama-3.2-3B\"  # You used this in your training code\n",
    "fine_tuned_model = \"llama-3.2-fine-tuned-model_ver5\"  # Your output dir from training\n",
    "\n",
    "# 🔁 Reload tokenizer and base model\n",
    "print(\"🔄 Loading base tokenizer and model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "base_model_reload = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Merging LoRA adapter with base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd37bb29ec4461b99933b142c13909c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d7881b7c294c049a03c00ec0b3e401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a004efad184d3fb1f8963b0ba04790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49bf73a35d32480eb3befe420a00e5e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7ed32ea1cb43698835be8991ae07ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/kevintran0310/llama-3.2-fine-tuned-model_ver5/commit/724514901e104dc957ae3b6c7456ea771b15372b', commit_message='Upload tokenizer', commit_description='', oid='724514901e104dc957ae3b6c7456ea771b15372b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/kevintran0310/llama-3.2-fine-tuned-model_ver5', endpoint='https://huggingface.co', repo_type='model', repo_id='kevintran0310/llama-3.2-fine-tuned-model_ver5'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 📎 Merge adapter\n",
    "print(\"🔗 Merging LoRA adapter with base model...\")\n",
    "model = PeftModel.from_pretrained(base_model_reload, fine_tuned_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# 💾 Save locally and push to HF Hub\n",
    "model_dir = \"llama-3.2-fine-tuned-model_ver5\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)\n",
    "\n",
    "# ☁️ Push to Hugging Face\n",
    "model.push_to_hub(model_dir, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(model_dir, use_temp_dir=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
