{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version improve the matching code from the answer to fuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mqtra0027\u001b[0m (\u001b[33mailecs-lab-students\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Login to Weights & Biases for experiment tracking\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/Pycharm Project/DUTA10K/wandb/run-20250507_065612-uqyiwzym</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ailecs-lab-students/Using%20Llma3.2%20to%20classify%20illicit%20content%20on%20online%20marketplace%20ver%202%20%28binary%20classification%29/runs/uqyiwzym' target=\"_blank\">fearless-dew-5</a></strong> to <a href='https://wandb.ai/ailecs-lab-students/Using%20Llma3.2%20to%20classify%20illicit%20content%20on%20online%20marketplace%20ver%202%20%28binary%20classification%29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ailecs-lab-students/Using%20Llma3.2%20to%20classify%20illicit%20content%20on%20online%20marketplace%20ver%202%20%28binary%20classification%29' target=\"_blank\">https://wandb.ai/ailecs-lab-students/Using%20Llma3.2%20to%20classify%20illicit%20content%20on%20online%20marketplace%20ver%202%20%28binary%20classification%29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ailecs-lab-students/Using%20Llma3.2%20to%20classify%20illicit%20content%20on%20online%20marketplace%20ver%202%20%28binary%20classification%29/runs/uqyiwzym' target=\"_blank\">https://wandb.ai/ailecs-lab-students/Using%20Llma3.2%20to%20classify%20illicit%20content%20on%20online%20marketplace%20ver%202%20%28binary%20classification%29/runs/uqyiwzym</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize a new Weights & Biases run\n",
    "run = wandb.init(\n",
    "    project='Using Llma3.2 to classify illicit content on online marketplace ver 2 (binary classification)', \n",
    "    job_type=\"training\", \n",
    "    resume=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "import evaluate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import (LlamaForSequenceClassification, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          DataCollatorWithPadding, \n",
    "                          EarlyStoppingCallback)\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                            precision_score, \n",
    "                            recall_score, \n",
    "                            f1_score, \n",
    "                            classification_report, \n",
    "                            confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import pipeline, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSONL file\n",
    "file_path = \"DUTA10K_final.jsonl\"\n",
    "df = pd.read_json(file_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train=3342 | Eval=417 | Test=419\n"
     ]
    }
   ],
   "source": [
    "# Define split sizes and split the DataFrame\n",
    "n = len(df)\n",
    "train_end = int(0.8 * n)\n",
    "eval_end  = train_end + int(0.1 * n)\n",
    "\n",
    "df_train = df.iloc[:train_end]\n",
    "df_eval  = df.iloc[train_end:eval_end]\n",
    "df_test  = df.iloc[eval_end:]\n",
    "\n",
    "print(f\"Train={len(df_train)} | Eval={len(df_eval)} | Test={len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DatasetDict from the pandas DataFrames\n",
    "ds = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train),\n",
    "    \"eval\":  Dataset.from_pandas(df_eval),\n",
    "    \"test\":  Dataset.from_pandas(df_test),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591fdc9af71747048d7c605402b3053a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3342 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33209663979c43e9a71d1d04caf9f61f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/417 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6040bf7061a448629ebac2bc947c3198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/419 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the base model tokenizer\n",
    "base_model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "MAX_LEN = 10000  # or 1,024 if you have the headroom\n",
    "\n",
    "# Preprocess function to tokenize the text data\n",
    "def preprocess(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        padding=\"max_length\"   # pad shorter examples up to exactly MAX_LEN\n",
    "    )\n",
    "\n",
    "# Apply preprocessing to the datasets\n",
    "tokenized = ds.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data Collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics functions\n",
    "metric = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred # eval_pred contains predictions and labels\n",
    "    preds = np.argmax(logits, axis=-1)# Get predicted class IDs\n",
    "    return metric.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd5f6e51b8f45e2b9762925c284c833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-3B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Configure BitsAndBytes for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load GEMMA‚Äë3 for sequence classification\n",
    "model = LlamaForSequenceClassification.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    num_labels=2,\n",
    "    id2label={0:\"non-illicit\",1:\"illicit\"},\n",
    "    label2id={\"non-illicit\":0,\"illicit\":1},\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Prepare the model for k-bit training (LoRA compatible)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.gradient_checkpointing_enable()# Enable gradient checkpointing to save memory during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA will adapt: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'score']\n"
     ]
    }
   ],
   "source": [
    "# Identify target modules for LoRA adaptation (all linear layers)\n",
    "SUPPORTED = (nn.Linear,)\n",
    "target_modules = [\n",
    "    name.split(\".\")[-1]\n",
    "    for name, m in model.named_modules()\n",
    "    if isinstance(m, SUPPORTED)\n",
    "]\n",
    "print(\"LoRA will adapt:\", target_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2138743/1616533681.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 97,261,568 || all params: 3,310,017,536 || trainable%: 2.9384\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA (Low-Rank Adaptation)\n",
    "lora_config = LoraConfig(\n",
    "    r=64, # LoRA rank\n",
    "    lora_alpha=32, # LoRA scaling factor\n",
    "    target_modules=target_modules, # Modules to apply LoRA to\n",
    "    lora_dropout=0.1, # Dropout probability for LoRA layers\n",
    "    bias=\"none\", # Do not apply bias to LoRA weights\n",
    "    task_type=\"SEQ_CLS\", # Sequence Classification task\n",
    ")\n",
    "\n",
    "# Get the PEFT (Parameter-Efficient Fine-Tuning) model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters() # Print the number of trainable parameters\n",
    "\n",
    "# Configure training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"llama3_binary_ver2\", # Output directory for checkpoints and logs\n",
    "    per_device_train_batch_size=1, # Batch size per GPU for training\n",
    "    per_device_eval_batch_size=1, # Batch size per GPU for evaluation\n",
    "    gradient_accumulation_steps=8, # Number of updates steps to accumulate before performing a backward/update pass\n",
    "    learning_rate=2e-5, # Initial learning rate for AdamW optimizer\n",
    "    num_train_epochs=8, # Total number of training epochs\n",
    "    eval_strategy=\"epoch\", # Evaluation is done at the end of each epoch\n",
    "    save_strategy=\"epoch\", # Model is saved at the end of each epoch\n",
    "    load_best_model_at_end=True, # Load the best model at the end of training\n",
    "    metric_for_best_model=\"accuracy\", # Metric to use to compare models\n",
    "    fp16=True, # Enable mixed precision training\n",
    "    logging_steps=50, # Log training loss and learning rate every 50 steps\n",
    "    report_to=[\"wandb\"], # Report metrics to Weights & Biases\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"eval\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    "    # callbacks=[early_stop],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3336' max='3336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3336/3336 31:32:38, Epoch 7/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.413400</td>\n",
       "      <td>0.377155</td>\n",
       "      <td>0.846523</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.310811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.239200</td>\n",
       "      <td>0.387714</td>\n",
       "      <td>0.868106</td>\n",
       "      <td>0.580153</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.513514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.232200</td>\n",
       "      <td>0.367212</td>\n",
       "      <td>0.875300</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.729730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.152200</td>\n",
       "      <td>0.403477</td>\n",
       "      <td>0.894484</td>\n",
       "      <td>0.671642</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.608108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.042600</td>\n",
       "      <td>0.498085</td>\n",
       "      <td>0.884892</td>\n",
       "      <td>0.641791</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.581081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.576772</td>\n",
       "      <td>0.889688</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.621622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.650193</td>\n",
       "      <td>0.889688</td>\n",
       "      <td>0.676056</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.648649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3336, training_loss=0.16215479258242413, metrics={'train_runtime': 113593.5562, 'train_samples_per_second': 0.235, 'train_steps_per_second': 0.029, 'total_flos': 4.66690721304576e+18, 'train_loss': 0.16215479258242413, 'epoch': 7.981448234590066})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('llama3_binary_ver2/tokenizer_config.json',\n",
       " 'llama3_binary_ver2/special_tokens_map.json',\n",
       " 'llama3_binary_ver2/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save trained model and tokenizer\n",
    "trainer.save_model(\"llama3_binary_ver2\")\n",
    "tokenizer.save_pretrained(\"llama3_binary_ver2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForSequenceClassification(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 3072)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=3072, out_features=2, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=3072, out_features=2, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure model & tokenizer are on the right device\n",
    "device = next(model.parameters()).device\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text: str):\n",
    "    # tokenize + move to device\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    ).to(device)\n",
    "    # forward\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits  # shape (1,2)\n",
    "    probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "    idx   = int(np.argmax(probs))\n",
    "    return {\n",
    "        \"label\":    id2label[idx],\n",
    "        \"score\":    float(probs[idx]),\n",
    "        \"all_probs\": { id2label[i]: float(probs[i]) for i in range(len(probs)) }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get predictions on the test set\n",
    "preds_output = trainer.predict(tokenized[\"test\"])\n",
    "y_true = preds_output.label_ids # True labels\n",
    "y_pred = np.argmax(preds_output.predictions, axis=-1) # Predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Compute & print metrics\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "rec  = recall_score(y_true, y_pred, zero_division=0)\n",
    "f1   = f1_score(y_true, y_pred, zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.8902\n",
      "Test Precision: 0.8393\n",
      "Test Recall:    0.5595\n",
      "Test F1:        0.6714\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " non‚Äëillicit       0.90      0.97      0.93       335\n",
      "     illicit       0.84      0.56      0.67        84\n",
      "\n",
      "    accuracy                           0.89       419\n",
      "   macro avg       0.87      0.77      0.80       419\n",
      "weighted avg       0.89      0.89      0.88       419\n",
      "\n",
      "Confusion Matrix:\n",
      "[[326   9]\n",
      " [ 37  47]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Accuracy:  {acc:.4f}\")\n",
    "print(f\"Test Precision: {prec:.4f}\")\n",
    "print(f\"Test Recall:    {rec:.4f}\")\n",
    "print(f\"Test F1:        {f1:.4f}\")\n",
    "\n",
    "# Full classification report + confusion matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"non‚Äëillicit\",\"illicit\"], zero_division=0))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñà‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñÖ‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñà‚ñà</td></tr><tr><td>eval/precision</td><td>‚ñÇ‚ñÉ‚ñÅ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñÑ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá</td></tr><tr><td>eval/runtime</td><td>‚ñÇ‚ñÉ‚ñÑ‚ñá‚ñÅ‚ñÖ‚ñÇ‚ñà</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñÑ‚ñÑ‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñÑ‚ñÑ‚ñÅ</td></tr><tr><td>test/accuracy</td><td>‚ñÅ</td></tr><tr><td>test/f1</td><td>‚ñÅ</td></tr><tr><td>test/loss</td><td>‚ñÅ</td></tr><tr><td>test/precision</td><td>‚ñÅ</td></tr><tr><td>test/recall</td><td>‚ñÅ</td></tr><tr><td>test/runtime</td><td>‚ñÅ</td></tr><tr><td>test/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>test/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñà‚ñá‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.88969</td></tr><tr><td>eval/f1</td><td>0.67606</td></tr><tr><td>eval/loss</td><td>0.65019</td></tr><tr><td>eval/precision</td><td>0.70588</td></tr><tr><td>eval/recall</td><td>0.64865</td></tr><tr><td>eval/runtime</td><td>459.0015</td></tr><tr><td>eval/samples_per_second</td><td>0.908</td></tr><tr><td>eval/steps_per_second</td><td>0.908</td></tr><tr><td>test/accuracy</td><td>0.89021</td></tr><tr><td>test/f1</td><td>0.67143</td></tr><tr><td>test/loss</td><td>0.41129</td></tr><tr><td>test/precision</td><td>0.83929</td></tr><tr><td>test/recall</td><td>0.55952</td></tr><tr><td>test/runtime</td><td>458.8334</td></tr><tr><td>test/samples_per_second</td><td>0.913</td></tr><tr><td>test/steps_per_second</td><td>0.913</td></tr><tr><td>total_flos</td><td>4.66690721304576e+18</td></tr><tr><td>train/epoch</td><td>7.98145</td></tr><tr><td>train/global_step</td><td>3336</td></tr><tr><td>train/grad_norm</td><td>0.03945</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0004</td></tr><tr><td>train_loss</td><td>0.16215</td></tr><tr><td>train_runtime</td><td>113593.5562</td></tr><tr><td>train_samples_per_second</td><td>0.235</td></tr><tr><td>train_steps_per_second</td><td>0.029</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fearless-dew-5</strong> at: <a href='https://wandb.ai/ailecs-lab-students/Using%20Llma3.2%20to%20classify%20illicit%20content%20on%20online%20marketplace%20ver%202%20%28binary%20classification%29/runs/uqyiwzym' target=\"_blank\">https://wandb.ai/ailecs-lab-students/Using%20Llma3.2%20to%20classify%20illicit%20content%20on%20online%20marketplace%20ver%202%20%28binary%20classification%29/runs/uqyiwzym</a><br> View project at: <a href='https://wandb.ai/ailecs-lab-students/Using%20Llma3.2%20to%20classify%20illicit%20content%20on%20online%20marketplace%20ver%202%20%28binary%20classification%29' target=\"_blank\">https://wandb.ai/ailecs-lab-students/Using%20Llma3.2%20to%20classify%20illicit%20content%20on%20online%20marketplace%20ver%202%20%28binary%20classification%29</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250507_065612-uqyiwzym/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finish the Weights & Biases run\n",
    "wandb.finish()\n",
    "model.config.use_cache = True # Set use_cache to True for inference after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, LlamaForSequenceClassification\n",
    "from peft import PeftModel\n",
    "\n",
    "# üîê Login to HuggingFace\n",
    "from getpass import getpass\n",
    "hf_token = getpass(\"Enter your HuggingFace token: \")\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading base tokenizer and model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a2d40fa11f442e9f9b56836fd2d47b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-3B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# üß† Base and fine-tuned model paths\n",
    "base_model = \"meta-llama/Llama-3.2-3B\"  # You used this in your training code\n",
    "fine_tuned_model = \"llama3_binary_ver2\"  # Your output dir from training\n",
    "\n",
    "# üîÅ Reload tokenizer and base model\n",
    "print(\"üîÑ Loading base tokenizer and model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "base_model_reload = LlamaForSequenceClassification.from_pretrained(\n",
    "    base_model,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Merging LoRA adapter with base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c4911df17d425c9f6e2c45664d5ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a023898e2ea64f95a96ef472746c267a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9e088fac904f34bfff82a8e45ad5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/2.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4975c6339cd249e898ded508d2c0a307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab5059478a44be082e3f1be51807cdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6452216db0ff43eca5b7570d61e6164a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/kevintran0310/llama3_binary_ver2/commit/47bc44c5a8840854c9dd1c9e4d46fb5812a58b8f', commit_message='Upload tokenizer', commit_description='', oid='47bc44c5a8840854c9dd1c9e4d46fb5812a58b8f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/kevintran0310/llama3_binary_ver2', endpoint='https://huggingface.co', repo_type='model', repo_id='kevintran0310/llama3_binary_ver2'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# üìé Merge adapter\n",
    "print(\"üîó Merging LoRA adapter with base model...\")\n",
    "model = PeftModel.from_pretrained(base_model_reload, fine_tuned_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# üíæ Save locally and push to HF Hub\n",
    "model_dir = \"llama3_binary_ver2\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)\n",
    "\n",
    "# ‚òÅÔ∏è Push to Hugging Face\n",
    "model.push_to_hub(model_dir, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(model_dir, use_temp_dir=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
